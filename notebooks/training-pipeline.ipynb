{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to filter out rare meddra terms\n",
    "min_occurence = 10\n",
    "\n",
    "with open('pt_counts.pkl', 'rb') as file:\n",
    "    tag_counts = pickle.load(file)\n",
    "    \n",
    "with open(\"spell_checker.pkl\", \"rb\") as file:\n",
    "    spell = pickle.load(file)\n",
    "\n",
    "def get_features(case):\n",
    "    \"\"\"\n",
    "    return effect_description, drugname, [sex, age, imc]\n",
    "    \n",
    "    Depends on your data format,\n",
    "        - effect_description is the free text written by the patient, it must be tokenized\n",
    "            (you can use the tokenize function above)\n",
    "        - drugname is simply the drug name, we used a spell checker trained on specific text (python library pyspellchecker==0.5.0)\n",
    "        - sex: can be encoded as an int (0-1)\n",
    "        - age and imc are given as numerical values (int of float and float)\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "'dataset.pkl' contains your dataset of features, it is a dict with unique key corresponding to each case.\n",
    "\"\"\"\n",
    "with open('dataset.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    \n",
    "\"\"\"\n",
    "'regex_match.pkl' contains meddra terms matched to each case using a regex engine.\n",
    "    it is a dict with the same key as for 'dataset.pkl'.\n",
    "\"\"\"\n",
    "with open('regex_match.pkl', 'rb') as file:\n",
    "    regex_match = pickle.load(file)\n",
    "\n",
    "\"\"\"\n",
    "'tags.pkl' contains the meddra tags that correspond to your dataset, it is a dict with the same key as for 'dataset.pkl'.\n",
    "    We only keep the most common terms (i.e with number of occurences greater than the min_occurence parameter.)\n",
    "\"\"\" \n",
    "with open('tags.pkl', 'rb') as file:\n",
    "    tags = pickle.load(file)\n",
    "    \n",
    "X = []\n",
    "Y = []\n",
    "re_match = []\n",
    "\"\"\"\n",
    "    We build the X and Y arrays from our features and tags.\n",
    "    X components are numeric vectors of features, it can be a mixture of text\n",
    "    vectorisation (using TF-IDF or any text embedding algorithm), numerical\n",
    "    features (age, weigh,...) and one hot encoding of categorical features (gender).\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "    /!\\ If you use a non pre trained text vectorization model, you should compute it on the train\n",
    "    sample after train-test split (next cell) to avoid introducing bias in your evaluation. Indeed,\n",
    "    if you compute for instance TF-IDF on the whole dataset (ie before splitting) test data will be\n",
    "    used for word frequency computation.\n",
    "\"\"\"\n",
    "for key, value in data.items():\n",
    "    X.append(value)\n",
    "    Y.append(tags[key])\n",
    "    re_match.append(regex_match[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test - train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test, regex_train, regex_test = train_test_split(X, Y, re_match, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags binarization and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = tag_binarizer.fit_transform(Y_train)\n",
    "Y_test = tag_binarizer.transform(Y_test)\n",
    "regex_test_bin = tag_binarizer.transform(regex_test)\n",
    "# we remove rare matched terms to be consistent with the ML approach\n",
    "regex_test_filtered = [[s for s in l if s in tag_counts.keys() and tag_counts[s] >= min_occurence] for l in regex_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Training a random forest classifier.\n",
    "    The parameters were obtained through grid search tuning method (see a few cells below).\n",
    "\"\"\"\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=4,\n",
    "    n_jobs=8\n",
    ")\n",
    "clf.fit(X_train_vec, Y_train)\n",
    "\n",
    "pred_test = clf.predict_proba(X_test_vec)\n",
    "\"\"\"\n",
    "    As we have several labels to predict, we flatten the prediction output to compute the evaluation metrics.\n",
    "\"\"\"\n",
    "pred_test_flat = numpy.vstack(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = OneVsRestClassifier(\n",
    "    LogisticRegression(\n",
    "        multi_class='ovr'\n",
    "    ),\n",
    "    n_jobs=8\n",
    ")\n",
    "logit.fit(X_train_vec, Y_train)\n",
    "pred_test_logit = logit.predict_proba(X_test_vec)\n",
    "pred_test_logit_flat = numpy.hstack(pred_test_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = OneVsRestClassifier(\n",
    "    SVC(probability=True),\n",
    "    n_jobs=8\n",
    ")\n",
    "svc.fit(X_train_vec, Y_train)\n",
    "pred_test_svc = svc.predict_proba(X_test_vec)\n",
    "pred_test_svc_flat = numpy.hstack(pred_test_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = OneVsRestClassifier(\n",
    "    LGBMClassifier(\n",
    "        max_depth=4,\n",
    "        n_estimators=200\n",
    "    ),\n",
    "    n_jobs=10\n",
    ")\n",
    "lgbm.fit(X_train_vec, Y_train)\n",
    "pred_test_lgbm = lgbm.predict_proba(X_test_vec)\n",
    "pred_test_lgbm_flat = numpy.hstack(pred_test_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Here we perform a basic hyperparameters tuning (grid search approach) to find a good set\n",
    "    of parameters for the LGBM model. We can do the same for Random Forests.\n",
    "    For a more sophisticated tuning method, we could use dedicated sklearn modules \n",
    "    (RandomizedSearchCV, GridSearchCV). \n",
    "\"\"\"\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "depth = [2, 4, 6, 10, 15]\n",
    "n_est = [10, 50, 100, 150, 200]\n",
    "best_auc = 0\n",
    "best_params = None\n",
    "best_lgbm = None\n",
    "for x in tqdm(itertools.product(depth, n_est)):\n",
    "    lgbm = OneVsRestClassifier(\n",
    "        LGBMClassifier(\n",
    "            max_depth=x[0],\n",
    "            n_estimators=x[1]\n",
    "        ),\n",
    "        n_jobs=10\n",
    "    )\n",
    "    lgbm.fit(X_train_vec, Y_train)\n",
    "    pred_test_lgbm = lgbm.predict_proba(X_test_vec)\n",
    "    # lgbm + regex\n",
    "    pred_test_lgbm_regex = pred_test_lgbm + regex_test_bin\n",
    "    pred_test_lgbm_regex = numpy.minimum(pred_test_lgbm_regex, numpy.ones(pred_test_lgbm_regex.shape))\n",
    "    pred_test_lgbm_regex_flat = numpy.hstack(pred_test_lgbm_regex)\n",
    "    \n",
    "    fpr_lgbm, tpr_lgbm, _ = roc_curve(Y_test.flatten('C'), pred_test_lgbm_regex_flat)\n",
    "    roc_auc_lgbm = auc(fpr_lgbm, tpr_lgbm)\n",
    "    if roc_auc_lgbm > best_auc:\n",
    "        best_auc = roc_auc_lgbm\n",
    "        best_params = x\n",
    "        best_lgbm = lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_lgbm = best_lgbm.predict_proba(X_test_vec)\n",
    "pred_test_lgbm_flat = numpy.hstack(pred_test_lgbm)\n",
    "\n",
    "# lgbm + regex\n",
    "\"\"\"\n",
    "    We use a very simple ensembling method for lgbm and regex. We add the prediction\n",
    "    vectors them apply a threshold to be sure the result is lower than 1.\n",
    "\"\"\"\n",
    "pred_test_lgbm_regex = pred_test_lgbm + regex_test_bin\n",
    "pred_test_lgbm_regex = numpy.minimum(pred_test_lgbm_regex, numpy.ones(pred_test_lgbm_regex.shape))\n",
    "pred_test_lgbm_regex_flat = numpy.hstack(pred_test_lgbm_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Plotting the curves.\n",
    "\"\"\"\n",
    "\n",
    "fpr, tpr, _ = roc_curve(Y_test.flatten('F'), pred_test_flat)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fpr_logit, tpr_logit, _ = roc_curve(Y_test.flatten('C'), pred_test_logit_flat)\n",
    "roc_auc_logit = auc(fpr_logit, tpr_logit)\n",
    "\n",
    "fpr_svc, tpr_svc, _ = roc_curve(Y_test.flatten('C'), pred_test_svc_flat)\n",
    "roc_auc_svc = auc(fpr_svc, tpr_svc)\n",
    "\n",
    "fpr_lgbm, tpr_lgbm, _ = roc_curve(Y_test.flatten('C'), pred_test_lgbm_flat)\n",
    "roc_auc_lgbm = auc(fpr_lgbm, tpr_lgbm)\n",
    "\n",
    "fpr_lgbm_re, tpr_lgbm_re, _ = roc_curve(Y_test.flatten('C'), pred_test_lgbm_regex_flat)\n",
    "roc_auc_lgbm_re = auc(fpr_lgbm_re, tpr_lgbm_re)\n",
    "\n",
    "fpr_re, tpr_re, _ = roc_curve(Y_test.flatten('C'), numpy.hstack(regex_test_bin))\n",
    "roc_auc_re = auc(fpr_re, tpr_re)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve random forest (area = %0.2f)' % roc_auc)\n",
    "plt.plot(fpr_logit, tpr_logit, color='green',\n",
    "         lw=lw, label='ROC curve logit (area = %0.2f)' % roc_auc_logit)\n",
    "plt.plot(fpr_svc, tpr_svc, color='red',\n",
    "         lw=lw, label='ROC curve SVM (area = %0.2f)' % roc_auc_svc)\n",
    "plt.plot(fpr_lgbm, tpr_lgbm, color='black',\n",
    "         lw=lw, label='ROC curve LGBM (area = %0.2f)' % roc_auc_lgbm)\n",
    "plt.plot(fpr_lgbm_re, tpr_lgbm_re, color='blue',\n",
    "         lw=lw, label='ROC curve LGBM regex (area = %0.2f)' % roc_auc_lgbm_re)\n",
    "plt.plot(fpr_re, tpr_re, color='gray',\n",
    "         lw=lw, label='ROC curve pure regex (area = %0.2f)' % roc_auc_re)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic ({})'.format(level))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_auc', dpi=None, facecolor='w', edgecolor='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision - recall per model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot precision, recall and F1 score for each model.\n",
    "\n",
    "We also compute and print contingency tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, t = precision_recall_curve(Y_test.flatten('C'), numpy.hstack(regex_test_bin))\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot([0]+list(t), p, color='darkorange',\n",
    "         lw=lw, label='precision')\n",
    "plt.plot([0]+list(t), r, color='green',\n",
    "         lw=lw, label='recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Precision - recall')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = 2 * (p * r) / (p + r)\n",
    "F1 = [x if x==x else 0 for x in F1]\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot([0]+list(t), F1, color='blue',\n",
    "         lw=lw, label='regex')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 score')\n",
    "plt.title('F1 score')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_regex = t[numpy.argmax(F1)]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test.flatten('C'), [0 if x < th_regex else 1 for x in numpy.hstack(regex_test_bin)]).ravel()\n",
    "max(F1), tn, fp, fn, tp, th_regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, t = precision_recall_curve(Y_test.flatten('F'), pred_test_flat)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot([0]+list(t), p, color='darkorange',\n",
    "         lw=lw, label='precision')\n",
    "plt.plot([0]+list(t), r, color='green',\n",
    "         lw=lw, label='recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Precision - recall')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = 2 * (p * r) / (p + r)\n",
    "F1 = [x if x==x else 0 for x in F1]\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot([0]+list(t), F1, color='blue',\n",
    "         lw=lw, label='rf')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 score')\n",
    "plt.title('F1 score')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.argmax(F1), F1[numpy.argmax(F1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_rf = t[numpy.argmax(F1)]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test.flatten('C'), [0 if x < th_rf else 1 for x in pred_test_flat]).ravel()\n",
    "max(F1), tn, fp, fn, tp, th_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, t = precision_recall_curve(Y_test.flatten('C'), pred_test_logit_flat)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot([0]+list(t), p, color='darkorange',\n",
    "         lw=lw, label='precision')\n",
    "plt.plot([0]+list(t), r, color='green',\n",
    "         lw=lw, label='recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Precision - recall')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = 2 * (p * r) / (p + r)\n",
    "F1 = [x if x==x else 0 for x in F1]\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot([0]+list(t), F1, color='blue',\n",
    "         lw=lw, label='logit')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 score')\n",
    "plt.title('F1 score')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_logit = t[numpy.argmax(F1)]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test.flatten('C'), [0 if x < th_logit else 1 for x in pred_test_logit_flat]).ravel()\n",
    "max(F1), tn, fp, fn, tp, th_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, t = precision_recall_curve(Y_test.flatten('C'), pred_test_svc_flat)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot([0]+list(t), p, color='darkorange',\n",
    "         lw=lw, label='precision')\n",
    "plt.plot([0]+list(t), r, color='green',\n",
    "         lw=lw, label='recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Precision - recall')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = 2 * (p * r) / (p + r)\n",
    "F1 = [x if x==x else 0 for x in F1]\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot([0]+list(t), F1, color='blue',\n",
    "         lw=lw, label='svc')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 score')\n",
    "plt.title('F1 score')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_svc = t[numpy.argmax(F1)]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test.flatten('C'), [0 if x < th_svc else 1 for x in pred_test_svc_flat]).ravel()\n",
    "max(F1), tn, fp, fn, tp, th_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, t = precision_recall_curve(Y_test.flatten('C'), pred_test_lgbm_flat)\n",
    "p_re, r_re, t_re = precision_recall_curve(Y_test.flatten('C'), pred_test_lgbm_regex_flat)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot([0]+list(t), p, color='darkorange',\n",
    "         lw=lw, label='precision', linestyle='--')\n",
    "plt.plot([0]+list(t), r, color='green',\n",
    "         lw=lw, label='recall', linestyle='--')\n",
    "plt.plot([0]+list(t_re), p_re, color='darkorange',\n",
    "         lw=lw, label='precision')\n",
    "plt.plot([0]+list(t_re), r_re, color='green',\n",
    "         lw=lw, label='recall')\n",
    "plt.plot([0]+list(t_rem), p_rem, color='darkorange',\n",
    "         lw=lw, label='precision', linestyle='dotted')\n",
    "plt.plot([0]+list(t_rem), r_rem, color='green',\n",
    "         lw=lw, label='recall', linestyle='dotted')\n",
    "plt.xlabel('Threshold')\n",
    "#plt.ylabel('True Positive Rate')\n",
    "plt.title('Precision - recall')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_re, r_re, t_re = precision_recall_curve(Y_test.flatten('C'), pred_test_lgbm_regex_flat)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot([0]+list(t_re)[:-1], p_re[:-1], color='darkorange',\n",
    "         lw=lw, label='precision')\n",
    "plt.plot([0]+list(t_re)[:-1], r_re[:-1], color='green',\n",
    "         lw=lw, label='recall')\n",
    "plt.xlabel('Threshold')\n",
    "#plt.ylabel('True Positive Rate')\n",
    "plt.title('Precision - recall')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('precision_recall', dpi=None, facecolor='w', edgecolor='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = 2 * (p * r) / (p + r)\n",
    "F1 = [x if x==x else 0 for x in F1]\n",
    "F1_re = 2 * (p_re * r_re) / (p_re + r_re)\n",
    "F1_re = [x if x==x else 0 for x in F1_re]\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot([0]+list(t), F1, color='blue',\n",
    "         lw=lw, label='lgbm', linestyle='--')\n",
    "plt.plot([0]+list(t_re)[:-1], F1_re[:-1], color='blue',\n",
    "         lw=lw, label='lgbm + regex')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 score')\n",
    "plt.title('F1 score')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_lgbm = t[numpy.argmax(F1)]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test.flatten('C'), [0 if x < th_lgbm else 1 for x in pred_test_lgbm_flat]).ravel()\n",
    "max(F1), tp, fn, fp, tn, th_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_re = t_re[numpy.argmax(F1_re)]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test.flatten('C'), [0 if x < th_re else 1 for x in pred_test_lgbm_regex_flat]).ravel()\n",
    "max(F1_re), tp, fn, fp, tn, th_re"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
