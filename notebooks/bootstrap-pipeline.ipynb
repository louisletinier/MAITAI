{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to filter out rare meddra terms\n",
    "min_occurence = 10\n",
    "\n",
    "with open('pt_counts.pkl', 'rb') as file:\n",
    "    tag_counts = pickle.load(file)\n",
    "    \n",
    "with open(\"spell_checker.pkl\", \"rb\") as file:\n",
    "    spell = pickle.load(file)\n",
    "\n",
    "def get_features(case):\n",
    "    \"\"\"\n",
    "    return effect_description, drugname, [sex, age, imc]\n",
    "    \n",
    "    Depends on your data format,\n",
    "        - effect_description is the free text written by the patient, it must be tokenized\n",
    "            (you can use the tokenize function above)\n",
    "        - drugname is simply the drug name, we used a spell checker trained on specific text (python library pyspellchecker==0.5.0)\n",
    "        - sex: can be encoded as an int (0-1)\n",
    "        - age and imc are given as numerical values (int of float and float)\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "'dataset.pkl' contains your dataset of features, it is a dict with unique key corresponding to each case.\n",
    "\"\"\"\n",
    "with open('dataset.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    \n",
    "\"\"\"\n",
    "'regex_match.pkl' contains meddra terms matched to each case using a regex engine.\n",
    "    it is a dict with the same key as for 'dataset.pkl'.\n",
    "\"\"\"\n",
    "with open('regex_match.pkl', 'rb') as file:\n",
    "    regex_match = pickle.load(file)\n",
    "\n",
    "\"\"\"\n",
    "'tags.pkl' contains the meddra tags that correspond to your dataset, it is a dict with the same key as for 'dataset.pkl'.\n",
    "    We only keep the most common terms (i.e with number of occurences greater than the min_occurence parameter.)\n",
    "\"\"\" \n",
    "with open('tags.pkl', 'rb') as file:\n",
    "    tags = pickle.load(file)\n",
    "    \n",
    "X = []\n",
    "Y = []\n",
    "re_match = []\n",
    "\"\"\"\n",
    "    We build the X and Y arrays from our features and tags.\n",
    "    X components are numeric vectors of features, it can be a mixture of text\n",
    "    vectorisation (using TF-IDF or any text embedding algorithm), numerical\n",
    "    features (age, weigh,...) and one hot encoding of categorical features (gender).\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "    /!\\ If you use a non pre trained text vectorization model, you should compute it on the train\n",
    "    sample after train-test split (next cell) to avoid introducing bias in your evaluation. Indeed,\n",
    "    if you compute for instance TF-IDF on the whole dataset (ie before splitting) test data will be\n",
    "    used for word frequency computation.\n",
    "\"\"\"\n",
    "for key, value in data.items():\n",
    "    X.append(value)\n",
    "    Y.append(tags[key])\n",
    "    re_match.append(regex_match[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(pred, true, model_name):\n",
    "    \"\"\"\n",
    "        Function for simulations results metrics computation and export.\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(true, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    p, r, t = precision_recall_curve(true, pred)\n",
    "    F1 = 2 * (p * r) / (p + r)\n",
    "    F1 = [x if x==x else 0 for x in F1]\n",
    "    \n",
    "    th = t[numpy.argmax(F1)]\n",
    "    tn, fp, fn, tp = confusion_matrix(true, [0 if x < th else 1 for x in pred]).ravel()\n",
    "    with open('result_bootstrap.txt', 'a') as file:\n",
    "        file.write(\n",
    "            '\\t'.join(\n",
    "                [\n",
    "                    model_name,\n",
    "                    str(max(F1)),\n",
    "                    str(tn),\n",
    "                    str(tp),\n",
    "                    str(fn),\n",
    "                    str(fp),\n",
    "                    str(roc_auc),\n",
    "                    'ROC_fpr',\n",
    "                    '$'.join([str(x) for x in fpr]),\n",
    "                    'ROC_tpr',\n",
    "                    '$'.join([str(x) for x in tpr])\n",
    "                ]\n",
    "            ) + '\\n'\n",
    "        )\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "n_split = 1000\n",
    "\"\"\"\n",
    "    We use ShuffleSplit to generate bootstrap samples indexes.\n",
    "\"\"\"\n",
    "rs = ShuffleSplit(n_splits=n_split, test_size=.1, random_state=0)\n",
    "for train_index, test_index in tqdm(rs.split(X, Y)):\n",
    "    \"\"\"\n",
    "        We build our train test sets\n",
    "    \"\"\"\n",
    "    X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "    Y_train, Y_test = [Y[i] for i in train_index], [Y[i] for i in test_index]\n",
    "    regex_train, regex_test = [regex_match[i] for i in train_index], [regex_match[i] for i in test_index]\n",
    "    \n",
    "    # we binarize the tags as well as the regex matches\n",
    "    Y_train = tag_binarizer.fit_transform(Y_train)\n",
    "    Y_test = tag_binarizer.transform(Y_test)\n",
    "    regex_test_bin = tag_binarizer.transform(regex_test)\n",
    "    regex_test_filtered = [[s for s in l if s in tag_counts.keys() and tag_counts[s] >= min_occurence] for l in regex_test]\n",
    "    \n",
    "    \"\"\"\n",
    "        We train and test each model on the current split and then export the results.\n",
    "    \"\"\"\n",
    "    # regex only\n",
    "    regex_test_flat = numpy.hstack(regex_test_bin)\n",
    "    export_results(regex_test_flat, Y_test.flatten('C'), 'regex')\n",
    "    \n",
    "    # train dataset\n",
    "    # lgbm \n",
    "    lgbm = OneVsRestClassifier(\n",
    "        LGBMClassifier(\n",
    "            max_depth=2,\n",
    "            n_estimators=50\n",
    "        ),\n",
    "        n_jobs=10\n",
    "    )\n",
    "    lgbm.fit(X_train_vec, Y_train)\n",
    "    pred_test = lgbm.predict_proba(X_test_vec)\n",
    "    pred_test_flat = numpy.hstack(pred_test)\n",
    "    export_results(pred_test_flat, Y_test.flatten('C'), 'lgbm')\n",
    "\n",
    "    # lgbm + regex\n",
    "    pred_test_regex = pred_test + regex_test_bin\n",
    "    pred_test_regex = numpy.minimum(pred_test_regex, numpy.ones(pred_test_regex.shape))\n",
    "    pred_test_regex_flat = numpy.hstack(pred_test_regex)\n",
    "    export_results(pred_test_regex_flat, Y_test.flatten('C'), 'lgbm + regex')\n",
    "\n",
    "    # Random Forest\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=4,\n",
    "        n_jobs=8\n",
    "    )\n",
    "    clf.fit(X_train_vec, Y_train)\n",
    "\n",
    "    pred_test = clf.predict_proba(X_test_vec)\n",
    "    pred_test_flat = numpy.vstack(pred_test)\n",
    "    pred_test_flat = [t[1] for t in pred_test_flat]\n",
    "    export_results(pred_test_flat, Y_test.flatten('F'), 'random_forests')\n",
    "\n",
    "    # SVM\n",
    "    svc = OneVsRestClassifier(\n",
    "        SVC(probability=True),\n",
    "        n_jobs=8\n",
    "    )\n",
    "    svc.fit(X_train_vec, Y_train)\n",
    "    pred_test = svc.predict_proba(X_test_vec)\n",
    "    pred_test_flat = numpy.hstack(pred_test)\n",
    "    export_results(pred_test_flat, Y_test.flatten('C'), 'svm')\n",
    "    \n",
    "    # logit\n",
    "    logit = OneVsRestClassifier(\n",
    "        LogisticRegression(\n",
    "            multi_class='ovr'\n",
    "        ),\n",
    "        n_jobs=8\n",
    "    )\n",
    "    logit.fit(X_train_vec, Y_train)\n",
    "    pred_test = logit.predict_proba(X_test_vec)\n",
    "    pred_test_flat = numpy.hstack(pred_test)\n",
    "    export_results(pred_test_flat, Y_test.flatten('C'), 'logit')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
